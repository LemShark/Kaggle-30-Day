{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_folds.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection with Boruta-sharp shows us, that not all features are usefull\n",
    "https://www.kaggle.com/lucamassaron/feature-selection-with-boruta-shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features, that we don't need\n",
    "\n",
    "df_test[\"cat1_A\"] = df_test[\"cat1\"].apply(lambda x: 1 if x == \"A\" else 0)\n",
    "df_test[\"cat8_C\"] = df_test[\"cat8\"].apply(lambda x: 1 if x == \"C\" else 0)\n",
    "df_test[\"cat8_E\"] = df_test[\"cat8\"].apply(lambda x: 1 if x == \"E\" else 0)\n",
    "\n",
    "df_test = df_test.drop(['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat6', 'cat7', 'cat9'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\n",
    "object_cols = [col for col in useful_features if col in (\"cat5\", \"cat8\") ]\n",
    "df_test = df_test[useful_features]\n",
    "\n",
    "for col in object_cols:\n",
    "    temp_df = []\n",
    "    temp_test_feat = None\n",
    "    for fold in range(5):\n",
    "        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n",
    "        xvalid = df[df.kfold == fold].reset_index(drop=True)\n",
    "        feat = xtrain.groupby(col)[\"target\"].agg(\"mean\")\n",
    "        feat = feat.to_dict()\n",
    "        xvalid.loc[:, f\"tar_enc_{col}\"] = xvalid[col].map(feat)\n",
    "        temp_df.append(xvalid)\n",
    "        if temp_test_feat is None:\n",
    "            temp_test_feat = df_test[col].map(feat)\n",
    "        else:\n",
    "            temp_test_feat += df_test[col].map(feat)\n",
    "    \n",
    "    temp_test_feat /= 5\n",
    "    df_test.loc[:, f\"tar_enc_{col}\"] = temp_test_feat\n",
    "    df = pd.concat(temp_df)\n",
    "    \n",
    "\n",
    "useful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\n",
    "object_cols = [col for col in useful_features if col in (\"cat5\", \"cat8\")]\n",
    "df_test = df_test[useful_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(trial):\n",
    "    fold = 0\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n",
    "    reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n",
    "    reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n",
    "\n",
    "    xtrain = df[df.kfold != fold].reset_index(drop=True)\n",
    "    xvalid = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    ytrain = xtrain.target\n",
    "    yvalid = xvalid.target\n",
    "\n",
    "    xtrain = xtrain[useful_features]\n",
    "    xvalid = xvalid[useful_features]\n",
    "\n",
    "    ordinal_encoder = preprocessing.OrdinalEncoder()\n",
    "    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n",
    "    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        random_state=42,\n",
    "        tree_method=\"gpu_hist\",\n",
    "        gpu_id=1,\n",
    "        predictor=\"gpu_predictor\",\n",
    "        n_estimators=7000,\n",
    "        learning_rate=learning_rate,\n",
    "        reg_lambda=reg_lambda,\n",
    "        reg_alpha=reg_alpha,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        max_depth=max_depth,\n",
    "    )\n",
    "    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n",
    "    preds_valid = model.predict(xvalid)\n",
    "    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(run, n_trials=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving predictions\n",
    "preds = np.mean(np.column_stack(final_predictions), axis=1)\n",
    "sample_submission.target = preds\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
